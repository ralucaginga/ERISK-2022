{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9895d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy emot flashtext contractions nrclex mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f79d689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Raluca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Raluca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle\n",
    "from time import time\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from emot.emo_unicode import UNICODE_EMOJI, UNICODE_EMOJI_ALIAS, EMOTICONS_EMO\n",
    "from flashtext import KeywordProcessor\n",
    "import nltk\n",
    "import re\n",
    "import contractions\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nrclex import NRCLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5b316ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data'\n",
    "antidepressant = os.path.join(data_folder, \"antidepressants.txt\")\n",
    "three_grams = os.path.join(data_folder, \"3-grams_suicide.txt\")\n",
    "five_grams = os.path.join(data_folder, \"5-grams_suicide.txt\")\n",
    "over_generalization = os.path.join(data_folder, \"over_generalization.txt\")\n",
    "psychoactive_drugs = os.path.join(data_folder, \"psychoactive_drugs.txt\")\n",
    "unpleasant_feeling = os.path.join(data_folder, \"unpleasant_feeling.txt\") \n",
    "nssi_words = os.path.join(data_folder, \"nssi_words.txt\")\n",
    "\n",
    "with open(antidepressant, \"r\") as f:\n",
    "    antidepressant_list = f.read().split(\"\\n\")\n",
    "    \n",
    "with open(three_grams, \"r\") as f:\n",
    "    three_grams_list = f.read().split(\"\\n\")\n",
    "    \n",
    "with open(five_grams, \"r\") as f:\n",
    "    five_grams_list = f.read().split(\"\\n\")\n",
    "    \n",
    "with open(over_generalization, \"r\") as f:\n",
    "    over_generalization_list = f.read().split(\"\\n\")\n",
    "    \n",
    "with open(psychoactive_drugs, \"r\") as f:\n",
    "    psychoactive_drug_list = f.read().split(\"\\n\")\n",
    "    \n",
    "with open(unpleasant_feeling, \"r\") as f:\n",
    "    unpleasant_feeling_list = f.read().split(\"\\n\")\n",
    "    \n",
    "with open(nssi_words, \"r\") as f:\n",
    "    nssi_list = f.read().split(\"\\n\")\n",
    "    \n",
    "temporal_past = [\"yesterday\", \"last\", \"before\", \"ago\", \"past\", \"back\", \"earlier\", \"later\"]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def basic_preprocess(text):\n",
    "  text = re.sub(r'http\\S+', ' ', text)\n",
    "  text = re.sub(r'\\[removed]', ' ', text)\n",
    "  text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "  text = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \", text)\n",
    "  return text\n",
    "\n",
    "def posting_time_level(dates):\n",
    "    count = 0\n",
    "    for date in dates:\n",
    "        if date.time().hour >= 0 and date.time().hour <= 7:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def split_on_window(sequence, limit=4):\n",
    "    results = []\n",
    "    split_sequence = sequence.lower().split()\n",
    "    iteration_length = len(split_sequence) - (limit - 1)\n",
    "    max_window_indicies = range(iteration_length)\n",
    "    for index in max_window_indicies:\n",
    "        results.append(' '.join(split_sequence[index:index + limit]))\n",
    "    return results\n",
    "\n",
    "def count_depressive_terms(text):\n",
    "    antidepress_count = len([sentence for sentence in text.split() if sentence.lower() in antidepressant_list])\n",
    "    three_grams_count = len([sentence for sentence in split_on_window(text, 3) if sentence in three_grams_list])\n",
    "    five_grams_count = len([sentence for sentence in split_on_window(text, 5) if sentence in five_grams_list])\n",
    "    overgeneralization_count = len([sentence for sentence in text.split() if sentence.lower() in over_generalization_list])\n",
    "    psychoactive_count = len([sentence for sentence in text.split() if sentence.lower() in psychoactive_drug_list])\n",
    "    unpleasant_feel_count = len([sentence for sentence in text.split() if sentence.lower() in unpleasant_feeling_list])\n",
    "    nssi_count = len([sentence for sentence in text.split() if sentence.lower() in nssi_list])\n",
    "    temporal_count = len([sentence for sentence in text.split() if sentence.lower() in temporal_past])\n",
    "    \n",
    "    return antidepress_count, three_grams_count, five_grams_count, overgeneralization_count, psychoactive_count, unpleasant_feel_count, nssi_count, temporal_count\n",
    "\n",
    "def features_pipeline(dates, text):\n",
    "    \n",
    "    \n",
    "    words_count = len(text.split())\n",
    "    punct_count = text.count('.') + text.count(',') + text.count(';') + text.count(':') + text.count('-')\n",
    "    questions_count = text.count('?')\n",
    "    exclamations_count = text.count('!')\n",
    "    capitalized_count = sum(map(str.isupper, text.split()))\n",
    "    \n",
    "    tagged_doc = nlp(text)\n",
    "\n",
    "    # Language Style\n",
    "    try:\n",
    "        adjective_count = len(list(map(lambda w: w.text, filter(lambda w: w.pos_ == 'ADJ', tagged_doc))))\n",
    "    except:\n",
    "        adjective_count = 0\n",
    "        \n",
    "    try:\n",
    "        verb_count = len(list(map(lambda w: w.text, filter(lambda w: (w.pos_ == 'AUX') | (w.pos_ == 'VERB'), tagged_doc))))\n",
    "    except:\n",
    "        verb_count = 0\n",
    "    \n",
    "    try:\n",
    "        noun_count = len(list(map(lambda w: w.text, filter(lambda w: w.pos_ == 'NOUN', tagged_doc)))) + len(list(map(lambda w: w.text, filter(lambda w: w.pos_ == 'PROPN', tagged_doc))))\n",
    "    except:\n",
    "        noun_count = 0\n",
    "        \n",
    "    try:\n",
    "        adverb_count = len(list(map(lambda w: w.text, filter(lambda w: w.pos_ == \"ADV\", tagged_doc))))\n",
    "    except:\n",
    "        adverb_count = 0\n",
    "        \n",
    "    try:\n",
    "        negation_count = len(list(map(lambda w: w.text, filter(lambda w: (w.pos_ == \"PART\" and w.morph.get(\"Polarity\") == [\"Neg\"]), tagged_doc))))\n",
    "    except:\n",
    "        negation_count = 0\n",
    "        \n",
    "    try:\n",
    "        formality_metric = (len(list(map(lambda w: w.text, filter(lambda w: w.pos_ == \"NOUN\", tagged_doc))))\n",
    "                               + len(list(map(lambda w: w.text, filter(lambda w: w.pos_ == \"PROPN\", tagged_doc))))\n",
    "                               + len(list(map(lambda w: w.text, filter(lambda w: w.pos_ == \"ADJ\", tagged_doc))))\n",
    "                               + len(list(map(lambda w: w.text, filter(lambda w: w.pos_ == \"ADP\", tagged_doc))))\n",
    "                               + len(list(map(lambda w: w.text, filter(lambda w: w.pos_ == \"DET\", tagged_doc))))\n",
    "                               - len(list(map(lambda w: w.text, filter(lambda w: w.pos_ == \"PRON\", tagged_doc))))\n",
    "                               - len(list(map(lambda w: w.text, filter(lambda w: (w.pos_ == 'AUX') | (w.pos_ == 'VERB'), tagged_doc))))\n",
    "                               - len(list(map(lambda w: w.text, filter(lambda w: w.pos_ == \"ADV\", tagged_doc))))\n",
    "                               - len(list(map(lambda w: w.text, filter(lambda w: w.pos_ == \"INTJ\", tagged_doc))))\n",
    "                               + 100) / 2\n",
    "    except:\n",
    "        formality_metric = 0\n",
    "        \n",
    "    try:   \n",
    "        trager_coefficient = len(list(map(lambda w: w.text, filter(lambda w: (w.pos_ == 'AUX') | (w.pos_ == 'VERB'), tagged_doc)))) / len(list(map(lambda w: w.text, filter(lambda w: w.pos_ == \"ADJ\", tagged_doc))))\n",
    "    except:\n",
    "        trager_coefficient = 0\n",
    "        \n",
    "    try:\n",
    "        readiness_to_action_coefficient = len(list(map(lambda w: w.text, filter(lambda w: (w.pos_ == 'AUX') | (w.pos_ == 'VERB'), tagged_doc)))) / (len(list(map(lambda w: w.text, filter(lambda w: w.pos_ == \"NOUN\", tagged_doc))))\n",
    "                               + len(list(map(lambda w: w.text, filter(lambda w: w.pos_ == \"PROPN\", tagged_doc)))))\n",
    "    except:\n",
    "        readiness_to_action_coefficient = 0\n",
    "        \n",
    "    try:\n",
    "        aggressiveness_coefficient = len(list(map(lambda w: w.text, filter(lambda w: (w.pos_ == \"VERB\" and w.morph.get(\"VerbForm\") == ['Part']), tagged_doc)))) / words_count\n",
    "    except:\n",
    "        aggressiveness_coefficient = 0\n",
    "        \n",
    "    try:\n",
    "        activity_index = len(list(map(lambda w: w.text, filter(lambda w: (w.pos_ == 'AUX') | (w.pos_ == 'VERB'), tagged_doc)))) / (len(list(map(lambda w: w.text, filter(lambda w: (w.pos_ == 'AUX') | (w.pos_ == 'VERB'), tagged_doc)))) \n",
    "                        + len(list(map(lambda w: w.text, filter(lambda w: w.pos_ == \"ADJ\", tagged_doc))))\n",
    "                        + len(list(map(lambda w: w.text, filter(lambda w: w.pos_ == \"ADP\", tagged_doc)))))\n",
    "    except:\n",
    "        activity_index = 0\n",
    "    \n",
    "    # User behaviour\n",
    "    time_level = posting_time_level(dates)\n",
    "    \n",
    "    \n",
    "    # Self-Preoccupation\n",
    "    try:\n",
    "        first_person_pron_count = len(list(map(lambda w: w.text, filter(lambda w: (w.pos_ == \"PRON\" and w.morph.get(\"Person\") == ['1']), tagged_doc))))\n",
    "    except:\n",
    "        first_person_pron_count = 0\n",
    "        \n",
    "    \n",
    "    # Reminiscicence & Sentiment\n",
    "    antidepress_count, three_grams_count, five_grams_count, overgeneralization_count, psychoactive_count, unpleasant_feel_count, nssi_count, temporal_count = count_depressive_terms(text)\n",
    "        \n",
    "    \n",
    "        \n",
    "    list_of_features = [words_count, punct_count, questions_count,\n",
    "                       exclamations_count, capitalized_count,\n",
    "                       adjective_count, verb_count, noun_count, adverb_count,\n",
    "                       negation_count, formality_metric,\n",
    "                       readiness_to_action_coefficient, aggressiveness_coefficient, activity_index,\n",
    "                       time_level, first_person_pron_count, antidepress_count, three_grams_count, five_grams_count,\n",
    "                       overgeneralization_count, psychoactive_count, unpleasant_feel_count,\n",
    "                       nssi_count, temporal_count]\n",
    "    \n",
    "    return list_of_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a36cd071",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing\n",
    "\n",
    "values = [\n",
    "        \"I have paranoia and depression. I have anxiety and I hate this life\",\n",
    "        \"I go to therapy. I have paranoia and go to psychologist\",\n",
    "        \"I like dogs, but I am hurting myself and biting my nails because of frustration\",\n",
    "        \"I was in a dark place and needed reassurance from my significant other that I was loved and wanted, and I then apologized for wanting and needing that. This was his response.\",\n",
    "        \"I had an early morning panic attack and texted my friend at 3:30 a.m… she was beyond awesome and helped me out even though I woke her up\",\n",
    "        \"Ukraine\"]\n",
    "dates = [[\"2021-02-20 12:00:20\", \"2021-02-21 00:20:12\"],\n",
    "        [\"2021-02-20 00:00:20\"],\n",
    "        [\"2021-02-10 12:00:20\"],\n",
    "        [\"2021-02-10 12:00:20\"],\n",
    "        [\"2021-02-10 12:00:20\"],\n",
    "        [\"2021-02-10 12:00:20\"]]\n",
    "\n",
    "clean_text = []\n",
    "features = []\n",
    "all_dates = []\n",
    "for text in values:\n",
    "    clean_text.append(basic_preprocess(text))\n",
    "    all_dates = [datetime.strptime(date, '%Y-%m-%d %H:%M:%S') for page in dates for date in page]\n",
    "    features.append(features_pipeline(all_dates, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081b4895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
