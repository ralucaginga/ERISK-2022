{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7f848b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import time\n",
    "import re\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"checkpoint-15546\")\n",
    "trainer = Trainer(model=model,\n",
    "                tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b42d3e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fast(text, model, tokenizer):  \n",
    "    #tokenize the text\n",
    "    start = time.time()\n",
    "    if len(text.split()) >= 450:\n",
    "        batches_no = len(text.split()) // 450 + 1\n",
    "        for i in range(batches_no):\n",
    "            text = ' '.join(text.split()[(i*450):(i+1)*450])\n",
    "            encodings = tokenizer(text, \n",
    "                          max_length=512, \n",
    "                          truncation=True, \n",
    "                          padding=True,\n",
    "                         return_tensors=\"pt\")\n",
    "            outputs = model(**encodings)\n",
    "            preds = outputs.logits\n",
    "            if int(preds.argmax(-1).numpy()[0]) == 1:\n",
    "                print(1)\n",
    "                print(time.time() - start)\n",
    "                return 1, float(torch.nn.functional.softmax(preds, dim=1).detach().numpy()[0][1])\n",
    "        print(0)\n",
    "        print(time.time() - start)\n",
    "        return 0, float(torch.nn.functional.softmax(preds, dim=1).detach().numpy()[0][1])\n",
    "    else:\n",
    "        encodings = tokenizer(text, \n",
    "                              max_length=512, \n",
    "                              truncation=True, \n",
    "                              padding=True,\n",
    "                             return_tensors=\"pt\")\n",
    "        outputs = model(**encodings)\n",
    "        preds = outputs.logits\n",
    "        print(int(preds.argmax(-1).numpy()[0]))\n",
    "        print(time.time() - start)\n",
    "        return int(preds.argmax(-1).numpy()[0]), float(torch.nn.functional.softmax(preds, dim=1).detach().numpy()[0][1])\n",
    "    \n",
    "    return preds, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "25272529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_preprocess(text):\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def predict_trainer(text):\n",
    "    start = time.time()\n",
    "    dataset = pd.DataFrame({'text': [basic_preprocess(text)]})\n",
    "    custom_dataset = Dataset.from_pandas(dataset)\n",
    "    tokenized_text = custom_dataset.map(lambda x: tokenizer(x['text'], truncation=True, max_length = 512), batched=True)\n",
    "    tokenized_text = tokenized_text.remove_columns(['text'])\n",
    "\n",
    "    trainer = Trainer(model=model,\n",
    "                    tokenizer=tokenizer)\n",
    "    output = np.argmax(trainer.predict(tokenized_text).predictions, axis=-1)\n",
    "    print(output)\n",
    "    print(time.time() - start)\n",
    "    return output\n",
    "\n",
    "def get_prediction1(values):\n",
    "    start = time.time()\n",
    "    values = basic_preprocess(values)\n",
    "    length_of_text = len(values.split())\n",
    "    result = []\n",
    "    if length_of_text >= 450:\n",
    "        decisions = []\n",
    "        scores = []\n",
    "        batches_no = length_of_text // 450 + 1\n",
    "        for i in range(batches_no):\n",
    "            text = ' '.join(values.split()[(i*450):(i+1)*450])\n",
    "            dataset = pd.DataFrame({'text': [text]})\n",
    "            custom_dataset = Dataset.from_pandas(dataset)\n",
    "            tokenized_text = custom_dataset.map(lambda x: tokenizer(x['text'],truncation=True, padding=\"max_length\",  max_length = 512), batched=True)\n",
    "            tokenized_text = tokenized_text.remove_columns(['text'])\n",
    "            trainer = Trainer(model=model,\n",
    "                            tokenizer=tokenizer)\n",
    "            dec = np.argmax(trainer.predict(tokenized_text).predictions, axis=-1)[0]\n",
    "            decisions.append(dec)\n",
    "            scores.append(tf.math.softmax(trainer.predict(tokenized_text).predictions, axis=-1).numpy()[0][dec])\n",
    "        if 1 in decisions:\n",
    "            decision = 1\n",
    "            score = scores[decisions.index(1)]\n",
    "        else:\n",
    "            decision = 0\n",
    "            score = scores[decisions.index(0)]\n",
    "    else:\n",
    "        dataset = pd.DataFrame({'text': [values]})\n",
    "        custom_dataset = Dataset.from_pandas(dataset)\n",
    "        tokenized_text = custom_dataset.map(lambda x: tokenizer(x['text'], truncation=True, padding=\"max_length\", max_length = 512), batched=True)\n",
    "        tokenized_text = tokenized_text.remove_columns(['text'])\n",
    "\n",
    "        trainer = Trainer(model=model,\n",
    "                        tokenizer=tokenizer)\n",
    "\n",
    "        decision = np.argmax(trainer.predict(tokenized_text).predictions, axis=-1)[0]\n",
    "        score = tf.math.softmax(trainer.predict(tokenized_text).predictions, axis=-1).numpy()[0][decision]\n",
    "    print(decision)\n",
    "    print(time.time() - start)\n",
    "    return decision, score\n",
    "\n",
    "def get_prediction2(values):\n",
    "    start = time.time()\n",
    "    values = basic_preprocess(values)\n",
    "    length_of_text = len(values.split())\n",
    "    if length_of_text >= 450:\n",
    "        batches_no = length_of_text // 450 + 1\n",
    "        for i in range(batches_no):\n",
    "            text = ' '.join(values.split()[(i * 450):(i + 1) * 450])\n",
    "            dataset = pd.DataFrame({'text': [text]})\n",
    "            custom_dataset = Dataset.from_pandas(dataset)\n",
    "            tokenized_text = custom_dataset.map(\n",
    "                lambda x: tokenizer(x['text'], truncation=True, padding=\"max_length\", max_length=512),\n",
    "                batched=True)\n",
    "            tokenized_text = tokenized_text.remove_columns(['text'])\n",
    "            dec = np.argmax(trainer.predict(tokenized_text).predictions, axis=-1)[0]\n",
    "            if int(dec) == 1:\n",
    "                print(1)\n",
    "                print(time.time() - start)\n",
    "                return int(dec), float(\n",
    "                    tf.math.softmax(trainer.predict(tokenized_text).predictions, axis=-1).numpy()[0][dec])\n",
    "        print(0)\n",
    "        print(time.time() - start)\n",
    "        return int(dec), float(tf.math.softmax(trainer.predict(tokenized_text).predictions, axis=-1).numpy()[0][dec])\n",
    "    else:\n",
    "        dataset = pd.DataFrame({'text': [values]})\n",
    "        custom_dataset = Dataset.from_pandas(dataset)\n",
    "        tokenized_text = custom_dataset.map(\n",
    "            lambda x: tokenizer(x['text'], truncation=True, padding=\"max_length\", max_length=512), batched=True)\n",
    "        tokenized_text = tokenized_text.remove_columns(['text'])\n",
    "\n",
    "        decision = np.argmax(trainer.predict(tokenized_text).predictions, axis=-1)[0]\n",
    "        score = tf.math.softmax(trainer.predict(tokenized_text).predictions, axis=-1).numpy()[0][decision]\n",
    "        print(decision)\n",
    "        print(time.time() - start)\n",
    "        return int(decision), float(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fca92dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.0633096694946289\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16fd67115a594372acae91058a1ae5ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "6.245408773422241\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10cf0d05401a415e9b8c82da908ffdbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1600/1620101017.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpredict_fast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mget_prediction1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mget_prediction2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1600/762017383.py\u001b[0m in \u001b[0;36mget_prediction2\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mtokenized_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0mdecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdecision\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecision\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   1564\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1565\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1566\u001b[1;33m         output = self.prediction_loop(\n\u001b[0m\u001b[0;32m   1567\u001b[0m             \u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Prediction\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1568\u001b[0m         )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mprediction_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   1628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1629\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1630\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1631\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m                 \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mprediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[0;32m   1743\u001b[0m                         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1744\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1746\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1747\u001b[0m                     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1494\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1496\u001b[1;33m         outputs = self.bert(\n\u001b[0m\u001b[0;32m   1497\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1498\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    964\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m         )\n\u001b[1;32m--> 966\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    967\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    565\u001b[0m                 )\n\u001b[0;32m    566\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 567\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    568\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    453\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 455\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    456\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m     ):\n\u001b[1;32m--> 386\u001b[1;33m         self_outputs = self.self(\n\u001b[0m\u001b[0;32m    387\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrelative_position_scores_query\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrelative_position_scores_key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m         \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    309\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m             \u001b[1;31m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "values = [\"I have paranoia and depression\", \"I enjoy going out and playing with my kids.\"]\n",
    "predict_fast(values[1], model, tokenizer)\n",
    "get_prediction1(values[1])\n",
    "get_prediction2(values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612338ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac1880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "542dff4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143f6b04c8cb4c49b4a41b81b77a3d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "values = [\"         Let's get real: they are not gonna add them. They already said they don't like game ending/changing killstreaks :(\",\n",
    "          \"         (A bit long for ELI5, so apologies) Each citizen needs 2 food to survive. If you have food yields (from buildings, working food tiles, or trade routes) greater than (2)(Population), the extra food gets added to your food basket each turn. Once this basket is filled, you get an extra citizen! This citizen can now work a new tile, giving you more food/production/gold, and the cycle continues. After each population growth, the food basket is empty and also gets a little bit bigger, which is why your cities grow much faster when small, and much slower when large. This also explains why some policies/buildings improve +growth, +excess food, or +food leftover (these are all different concepts). Your production is directly tied to your citizens (who work production tiles or act as specialists) and your buildings. If you have 1 citizen, and he works a mine for 3 production, each turn you can produce 3 hammers. If a building costs 30 hammers, it will take 10 turns. If your population grows to 2, and now you can work 2 mines, you produce 6 hammers per turn, and your building is now built in 5 turns. As you can easily see, the more citizens you have to provide production (either by working tiles or as specialists), the greater your production each turn is, and the faster you can build buildings. The actual formulas vary greatly based on the tiles you can work, the improvements your workers make, and the buildings in the city. An easy way to understand this mechanic is, when your city first grows to Population 2, open up your city management screen (by clicking on the city). Then click Citizen Management in the upper right corner, and move your citizen around to different tiles. You will see direct changes to however long the building in your production queue has left, and you will see changes to 'Turns to Next Citizen.' Now take what you know for how 1 citizen moved around changes this, and multiply it by your entire populace for later in the game. Each game has a trade-off decision you have to make. Typically 'production-focused' tiles like mines will help you build early buildings much faster, but your cities will grow slower (and thus you have fewer citizens to work tiles). Alternatively, you can work farms to grow your city quickly, but your buildings will take much longer to build (until you have enough citizens to work production tiles in addition to food tiles). Since population drives several other key resources (like gold from trade routes and science), you generally want to do everything possible to get your cities as large as your happiness allows. Typically your production keeps up as your cities grow (assuming you settled near a few hills or forests), but you may want to manually adjust your citizens every so often to make sure your production isn't too low. Alternatively, if your cities aren't growing (and you want them to), you may need to re-assign citizens to food tiles at the expense of production.\",\n",
    "    \"    Update 3:18 AM, Big Ten logo is getting blued out     As title said, Big Ten logo is getting fucked up. See if you can do anything to help. Also, we're trying to build up a megathread at UIUC, found [here](https://www.reddit.com/r/UIUC/comments/6354dh/megathread_for_rplace/) if you want to help out\",\n",
    "         \"    blender with rust source bindings     is it possible to develop addons in rustlang? and intergrate it with blender via python?\",\n",
    "         \"    A quick question about the physics engine.     I searched around a bit, and wasn't able to find an answer to this question, though maybe I'm just bad at wording things in search engines. A while back, presumably after a past update, my physics started to get a bit weird. Everything seemed to lose most of its mass, to the point that I'm beginning to suspect that Appalachia is actually on Mars. Ragdolls, thrown grenades and mines, and basically anything else physics driven has become incredibly floaty, which wouldn't be a big deal other than that it messes up the timing and tragectory of my grenades. Is this a problem for everyone or is it just me? Is this Bethesda's problem or is there something I can do about it? For reference I'm playing on Xbox, and am up to date with all the latest updates.\",\n",
    "         \"         World to me is scary. Expectations from parents and society are making me feel like there is no meaning to life. I am disconnected with everything with zero interests. Maybe it's quarter life crisis maybe depression. I am in a great conflict with stoicism and life in general.\",\n",
    "         \"    Deja fucking vu     Didnt play poker for a week. Started to feel pretty good. Then I had the brilliant idea bc I got a little money to deposit $800 and play. Well I lost that in an hour and really didnt have a chance In hell to win. 2 outers, runner runner and a few other hands and there goes that. Im sick again. I dont want to have this feeling ever again. Im so done with this lifestyle and insanity. Its pure fucking evil and has to stop. There is not one thing good that comes from gambling. It makes me miserable, stressed, isolated, broke, scared, depressed, sad, angry. Its a brutal addiction and has really fucked my life up good. Im so fucking pissed at myself that I lost that money when money is so tight and Im barely getting by. -800$ in an hour. Really? Pure fucking stupidity. The whole time Im playing I know Im gonna lose bc its destiny I lose and lose and lose and lose and lose and lose some more so I get in enough pain to stop. When I stop for good thats when I finally win.\"]\n",
    "\n",
    "def basic_preprocess(text):\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    return text\n",
    "\n",
    "dataset = pd.DataFrame({'text': [basic_preprocess(text) for text in values]})\n",
    "custom_dataset = Dataset.from_pandas(dataset)\n",
    "tokenized_text = custom_dataset.map(lambda x: tokenizer(x['text'], truncation=True, padding=\"max_length\", max_length = 512), batched=True)\n",
    "tokenized_text = tokenized_text.remove_columns(['text'])\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                tokenizer=tokenizer)\n",
    "    \n",
    "print(np.argmax(trainer.predict(tokenized_text).predictions, axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f0eae1",
   "metadata": {},
   "source": [
    "### Get prediction method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1ebffa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(values):\n",
    "    values = basic_preprocess(values)\n",
    "    length_of_text = len(values.split())\n",
    "    result = []\n",
    "    if length_of_text >= 400:\n",
    "        decisions = []\n",
    "        scores = []\n",
    "        batches_no = length_of_text // 400 + 1\n",
    "        for i in range(batches_no):\n",
    "            text = ' '.join(values.split()[(i*400):(i+1)*400])\n",
    "            dataset = pd.DataFrame({'text': [text]})\n",
    "            custom_dataset = Dataset.from_pandas(dataset)\n",
    "            tokenized_text = custom_dataset.map(lambda x: tokenizer(x['text'], max_length = 512), batched=True)\n",
    "            tokenized_text = tokenized_text.remove_columns(['text'])\n",
    "            trainer = Trainer(model=model,\n",
    "                            tokenizer=tokenizer)\n",
    "            dec = np.argmax(trainer.predict(tokenized_text).predictions, axis=-1)[0]\n",
    "            decisions.append(dec)\n",
    "            scores.append(tf.math.softmax(trainer.predict(tokenized_text).predictions, axis=-1).numpy()[0][dec])\n",
    "        if 1 in decisions:\n",
    "            decision = 1\n",
    "            score = scores[decisions.index(1)]\n",
    "        else:\n",
    "            decision = 0\n",
    "            score = scores[decisions.index(0)]\n",
    "    else:\n",
    "        dataset = pd.DataFrame({'text': [values]})\n",
    "        custom_dataset = Dataset.from_pandas(dataset)\n",
    "        tokenized_text = custom_dataset.map(lambda x: tokenizer(x['text'], truncation=True, padding=\"max_length\", max_length = 512), batched=True)\n",
    "        tokenized_text = tokenized_text.remove_columns(['text'])\n",
    "\n",
    "        trainer = Trainer(model=model,\n",
    "                        tokenizer=tokenizer)\n",
    "\n",
    "        decision = np.argmax(trainer.predict(tokenized_text).predictions, axis=-1)[0]\n",
    "        score = tf.math.softmax(trainer.predict(tokenized_text).predictions, axis=-1).numpy()[0][decision]\n",
    "    return decision, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "92f4c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_round1 = [{\"redditor\": 338, \"content\": \"    Getting Sober Didn't Lift my Depression. (18 months sober\", \"date\": \"2014-12-12T04:21:13.000+0000\", \"id\": 168996, \"title\": \"    Copy the Reindeer\", \"number\": 1, \"nick\": \"subject8081\"},\n",
    "    {\"redditor\": 339, \"content\": \"    This post is all over the place, but I felt like sharing my story others can relate and no that they really are not alone! Before I start: Please don't get me wrong and think I come from some rich family and my parents are Doctors and giving me thousands in allowance, that is now true. I work hard for every dollar I make! I am thankful that I found and still have a very well paying job and it will take me a year to get back to where I was when I started gambling, but the pain is strong just like someone who lost it all with much less saved up. I just turned 21 and I am just hit rock bottom, or so I thought I did. Cash Advance - the worse feature of a credit card for a gambler. You see I thought once I hit $0 in the bank last week that would be all, I walked out of the casino in great spirits and ready to start a new chapter! Most people would be ready to kill themselves but I was the opposite, I was so happy and felt like some kind of burden was taken off my shoulders and as I was driving home I felt like a million bucks, even though I had $0 to my name! This high lasted a few days until I got paid and that urge came back to go, which I did....I went with $4,000 and at what point was up $8,000! Of course as we all know ''the house always wins'' and I ended up losing all of that $4,000 and then proceeded to take out $3,000 in cash advance...as you know I lost that , too. Before gambling I had saved over $60,000 was living fine , life was moving. I got a nice car, which thankfully I still have. Money was just a number like the numbers on the clock. I was thinking of moving out, getting a nice place that would cost $3,000 a month, get nice furniture have a game room, you get the point, living the good life. I was always looking for more ways to make money and gambling never crossed my mind. I have friends who always asked me to go to the casino with them, but it never interested me for some odd reason. One day I was bored and in the area of the casino.... A big mistake I made that day: I walked into the casino, something I wish to this day I would have never done. But what can a $500 loss do already? I HAD so much saved up! As you can imagine, I went in and lost that $500, what repeated after was relatable to anyone who is struggling with gambling, I wasted over $2k that day but that didn't phase me at all, I left and went to my hotel. Next morning comes and I just have this crazy urge to go back I give in go back and lose another $2k. Now for about a week I was fine as I was so occupied with work after vacation that i didn't even remember #160;about the losses but then the following weekend I went again and lost $5k in a single night. Now keep in mind you generally do not lose $1k at one time, it is over the course of a few machines. Going once a week led to going 5-7 times a week losing $500-$1k each time! I did enjoy myself there meeting a whole bunch of interesting characters that wasted even more money than I have but as you leave reality sets back in to place. There is no concept of money once you step food into a casino, any sense of value is outside those casino doors. I refused to buy myself a pair of shoes because they were $5 more than last time, yet I go to the casino and gamble 10x that! Around the time I started going to the casino I was in a relationship, things were rocky and we ended up breaking it up. I was devastated and shifted my focus to the casino. The timing of me going to the casino to which I had never gone before, and breaking up a month later worsened the situation a lot. I am not one to blame anyone or thing but myself! I am grown, and make my own decisions, I chose to let it get out of control and now I am suffering from this depression. I used to be the one helping other and now I have to now borrow $400 to pay a bill off that way I do not get charge interest for a late fee, how the wheel has turned. It makes you feel dead inside, you lose all life to you. Depression comes and goes, but when it is here, it hits hard! My biggest issue is that I am afraid of myself, I lose total control and go crazy! I have lost 4k in a matter of 2 hours! That is insane! I have told myself so many time that I am done, but I kept going back. At this point I have to pay off the 3k for the Cash Advance I took out yesterday. Learning the value of money is hard. I have trouble falling asleep, I wake up sad. I push through the day, sad and grumpy, and get my work done. I am just existing at this point, the spark has left and is waiting to be reignited. It will take 2-3 months for me to look at my bank and smile again, but I did this to myself and am now paying the consequence. I feel like writing this out helps me a bit and I truly hope someone will see this and know we can get through this. Has anyone else hit rock bottom and felt great, then a few days later it hit you like a ton of bricks? Any advice for me? Thanks in advance!\", \"date\": \"2013-10-10T13:17:01.000+0000\", \"id\": 169297, \"title\": \"\", \"number\": 1, \"nick\": \"subject2621\"},\n",
    "    {\"redditor\": 340, \"content\": \"    I have a question about being a visitor in Nioh(Random encounters)\", \"date\": \"2017-05-09T17:01:50.000+0000\", \"id\": 169531, \"title\": \"    Nioh - Become a visitor\", \"number\": 1, \"nick\": \"subject992\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e37fd67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44da04a267b43c59b11bf4c510aaae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.9821879\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5047e46ccff74b34a633afaffc75c3f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade4f59ac11c43af8f3582f42944ae76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374f065339df45018134653ffcf60e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1]\n",
      "1 0.9976046\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d20bcd3723405d9ce46f3bca0e3756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9991805\n"
     ]
    }
   ],
   "source": [
    "########### Should be modified\n",
    "\n",
    "for user in data_round1:\n",
    "    decision, score = get_prediction(user['content'])\n",
    "    print(decision, score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529273ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
